{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1kpMeCrUFd5",
        "outputId": "75f5e381-8826-4090-dc21-d5707f2849f2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "by1XmcLGD1nB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import keras\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = keras.utils.get_file(\"/content/paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\")\n",
        "style_reference_image_path = keras.utils.get_file(\n",
        "    \"/content/starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n",
        ")"
      ],
      "metadata": {
        "id": "mBG_rhIZL5qs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.vgg19(pretrained=True).features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zyyFe20F9-J",
        "outputId": "148df93c-20ca-44ed-b1e6-efb5f2708633"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VGG, self).__init__()\n",
        "\n",
        "    self.model = models.vgg19(pretrained=True).features[:29]\n",
        "    self.chosen_features = [\"0\", \"5\", \"10\", \"19\" ,\"28\"]\n",
        "\n",
        "  def forward(self, x):\n",
        "    features=[]\n",
        "\n",
        "    # for layer_num, layer in enumerate(self.model):\n",
        "    #   if str(layer_num) in self.chosen_features:\n",
        "    #     features.append(layer(x))\n",
        "    # return features\n",
        "\n",
        "    for layer_num, layer in enumerate(self.model):\n",
        "      x=layer(x)\n",
        "      if str(layer_num) in self.chosen_features:\n",
        "        features.append(x)\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "iZKtwMVRGWmO"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "img_dim = 356\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((img_dim, img_dim)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_image(img_path):\n",
        "  img = Image.open(img_path)\n",
        "  img = loader(img).unsqueeze(0)\n",
        "  return img.to(device)\n"
      ],
      "metadata": {
        "id": "G-OvEADhMc8u"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_img = load_image(\"/content/paris.jpg\")\n",
        "style_img = load_image(\"/content/starry_night.jpg\")"
      ],
      "metadata": {
        "id": "GOk9ZLP7Isew"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = original_img.clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "4qBmSHVlOGMu"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = 6000\n",
        "lr = 0.001\n",
        "alpha = 1\n",
        "beta = 0.01"
      ],
      "metadata": {
        "id": "2X-FoLI9OPHS"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([generated], lr=lr)"
      ],
      "metadata": {
        "id": "vWGHySPqOVLI"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG().to(device)\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_G13ydxObCb",
        "outputId": "f64ca771-cae0-42b3-d82b-50cc93db8e22"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(total_steps):\n",
        "  generated_features = model(generated)\n",
        "  original_img_features = model(original_img)\n",
        "  style_features = model(style_img)\n",
        "\n",
        "  style_loss=0\n",
        "  content_loss = 0\n",
        "\n",
        "  for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
        "    batch_size, channel, height, width = gen_feature.shape\n",
        "    content_loss += torch.mean((orig_feature - gen_feature)**2)\n",
        "\n",
        "    gen_gram = gen_feature.view(channel, height*width).mm(\n",
        "        gen_feature.view(channel, height*width).t()\n",
        "    )\n",
        "\n",
        "    style_gram = style_feature.view(channel, height*width).mm(\n",
        "        style_feature.view(channel, height*width).t()\n",
        "    )\n",
        "\n",
        "    style_loss += torch.mean((style_gram - gen_gram) ** 2)\n",
        "\n",
        "  total_loss = (beta*style_loss) + (alpha*content_loss)\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step%200==0:\n",
        "    print(f\"Loss at step-{step} = {total_loss.item()}\")\n",
        "    save_image(generated, f\"/content/save-{step}.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOkG3cJ8PRyN",
        "outputId": "189a9f70-cc0b-4e57-88ac-b20a4f0f8490"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step-0 = 667964.125\n",
            "Loss at step-200 = 30273.453125\n",
            "Loss at step-400 = 12338.76953125\n",
            "Loss at step-600 = 5605.4306640625\n",
            "Loss at step-800 = 3428.556640625\n",
            "Loss at step-1000 = 2662.79150390625\n",
            "Loss at step-1200 = 2266.47802734375\n",
            "Loss at step-1400 = 1998.0616455078125\n",
            "Loss at step-1600 = 1793.9908447265625\n",
            "Loss at step-1800 = 1631.442138671875\n",
            "Loss at step-2000 = 1495.5675048828125\n",
            "Loss at step-2200 = 1379.499755859375\n",
            "Loss at step-2400 = 1277.97265625\n",
            "Loss at step-2600 = 1188.7489013671875\n",
            "Loss at step-2800 = 1110.1407470703125\n",
            "Loss at step-3000 = 1040.2374267578125\n",
            "Loss at step-3200 = 977.2183227539062\n",
            "Loss at step-3400 = 920.3029174804688\n",
            "Loss at step-3600 = 869.2216186523438\n",
            "Loss at step-3800 = 823.2390747070312\n",
            "Loss at step-4000 = 781.6322021484375\n",
            "Loss at step-4200 = 744.1322631835938\n",
            "Loss at step-4400 = 709.6589965820312\n",
            "Loss at step-4600 = 679.482177734375\n",
            "Loss at step-4800 = 651.6325073242188\n",
            "Loss at step-5000 = 627.5870971679688\n",
            "Loss at step-5200 = 605.4299926757812\n",
            "Loss at step-5400 = 587.197998046875\n",
            "Loss at step-5600 = 569.7498779296875\n",
            "Loss at step-5800 = 552.973388671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_image(generated, \"/content/save-6000.jpg\")\n"
      ],
      "metadata": {
        "id": "j714TYRiY_Lc"
      },
      "execution_count": 125,
      "outputs": []
    }
  ]
}